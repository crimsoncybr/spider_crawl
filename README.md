# üï∑Ô∏è spider_crawl Web Crawler

> A real-time, keyword-aware Bash crawler with optional Dark Web support üåêüß†

---

### üöÄ Features

- üì• Downloads and parses HTML from any public website
- üîé Searches for keyword matches in page content and links
- üåê Supports both surface web and `.onion` (Tor) sites
- üïµÔ∏è Rotates User-Agents to mimic human browsing
- üìä Live terminal dashboard with:
  - Total URLs crawled
  - Keywords matched
  - Current target
  - Recent match results
- üóÇ Organizes downloads by domain name
- üß† Intelligent delay between requests (1‚Äì15 sec, randomized)

---
![Screenshot 2025-04-18 135258](https://github.com/user-attachments/assets/1453787b-bb0d-4fa0-844e-35d5eb483327)

---

### üåå Dark Web Support

To switch between Surface Web and Dark Web crawling:

Edit this section in the script (`SETUP_CONFIG()`):

```bash
# WGT="torrify wget"  # darkweb mode (requires Tor)
WGT="wget"            # normal mode
```

> üí° **Make sure Tor is running** (e.g., via `tor` or Tor Browser) when using darkweb mode.

To install Tor + Torrify:
```bash
sudo apt install tor torsocks
```

---

### üß± Directory Structure

```
.
‚îú‚îÄ‚îÄ spider_crawl.sh           # The main crawler script
‚îú‚îÄ‚îÄ urls/
‚îÇ   ‚îî‚îÄ‚îÄ urls                  # Seed list of URLs to crawl
‚îú‚îÄ‚îÄ search/
‚îÇ   ‚îî‚îÄ‚îÄ keywords.txt          # List of keywords to detect
‚îú‚îÄ‚îÄ info/
‚îÇ   ‚îú‚îÄ‚îÄ visited_urls.txt
‚îÇ   ‚îú‚îÄ‚îÄ match_counter.txt
‚îÇ   ‚îú‚îÄ‚îÄ keywords_found_counter.txt
‚îÇ   ‚îú‚îÄ‚îÄ matches.txt
‚îÇ   ‚îú‚îÄ‚îÄ match_output.tmp
‚îÇ   ‚îî‚îÄ‚îÄ temp_links
‚îú‚îÄ‚îÄ downloaded_pages/         # HTML files saved by domain
‚îî‚îÄ‚îÄ README.md
```

---

### ‚öôÔ∏è Usage

1. **Clone the repo**
   ```bash
   git clone https://github.com/crimsoncybr/spider_crawl.git
   cd spider_crawl
   ```

2. **Add initial URL(s)**
   ```bash
   echo "https://example.com" >> urls/urls
   ```

3. **Add keyword(s)**
   ```bash
   echo "cyber" >> search/keywords.txt
   ```

4. **Run the crawler**
   ```bash
   chmod +x spider_crawl.sh
   ./spider_crawl.sh
   ```

---

### üìã Requirements

- Bash 4+
- Tools: `wget`, `awk`, `grep`, `md5sum`, `sort`, `torsocks` (optional)
- OS: Linux or macOS (recommended)

---

### ‚ö†Ô∏è Disclaimer

This script is for educational and ethical use only.  
‚úÖ Always ensure you have permission before crawling or indexing any domain.

---

**Author:** Dean  
**License:** MIT  
**Stars Welcome!** ‚≠ê
